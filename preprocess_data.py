# -*- coding: utf-8 -*-
"""preprocess_data

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Svqgwol6WTLoNbbhsc8TuqsYn5pnfU11
"""

# preprocess_data.py
import pandas as pd
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import joblib
import os
import re

def clean_text(text):
    """Utility function to clean text by removing special characters and converting to lowercase."""
    if isinstance(text, str):
        text = re.sub(r'[^\w\s]', '', text) # Remove punctuation
        text = text.lower() # Convert to lowercase
        return text
    if isinstance(text, (int, float, bool)): # Convert numbers/booleans to string
        return str(text).lower()
    return ""

def create_content_soup(df_row):
    """Combines relevant features from the provided columns into a single string for TF-IDF."""
    # Using the most relevant textual columns from the user's list
    soup_parts = [
        clean_text(df_row.get('reviewText', '')),
        clean_text(df_row.get('criticName', '')),
        clean_text(df_row.get('publicatioName', '')), # Ensure this matches your CSV's column name
        clean_text(df_row.get('scoreSentiment', ''))
        # Consider adding isTopCritic as a textual tag if useful:
        # if df_row.get('isTopCritic', False): # Make sure 'isTopCritic' is in relevant_cols_for_processing
        #     soup_parts.append("topcritic")
    ]
    return ' '.join(filter(None, soup_parts)).strip()

def main():
    print("Step 1: Loading data...")
    data_dir = 'data'
    # CRITICAL CHANGE: Pointing to the reviews CSV file, as it likely contains
    # 'id', 'reviewText', 'criticName', 'publicatioName', 'scoreSentiment'.
    # The 'id' column in this file should ideally be a movie identifier to link reviews to movies.
    # If 'id' here is a unique review ID, then each review will be treated as a distinct item.
    movies_file = os.path.join(data_dir, 'rotten_tomatoes_movie_reviews-1.csv')

    if not os.path.exists(movies_file):
        print(f"Error: Data file not found at {movies_file}")
        print(f"Please ensure '{os.path.basename(movies_file)}' is in the '{data_dir}' subfolder.")
        return

    try:
        movies_df_full = pd.read_csv(movies_file)
    except Exception as e:
        print(f"Error loading CSV file {movies_file}: {e}")
        return

    print(f"Initial movies_df_full shape: {movies_df_full.shape}")
    print(f"Columns in loaded CSV '{os.path.basename(movies_file)}': {movies_df_full.columns.tolist()}")

    # --- Data Cleaning and Selection ---
    # Columns to select: 'id' for identifier, and others for the content soup.
    relevant_cols_for_processing = [
        'id',               # Expecting this to be the movie/item identifier from the reviews file
        'reviewText',
        'criticName',
        'publicatioName',   # Make sure this matches the exact column name in your CSV
        'scoreSentiment',
        'isTopCritic'       # Included for potential use in soup
    ]

    # Check which of these columns actually exist in the loaded DataFrame
    actual_existing_cols = [col for col in relevant_cols_for_processing if col in movies_df_full.columns]

    if 'id' not in movies_df_full.columns:
        print(f"Error: The crucial 'id' column (expected to be movie/item identifier) was not found in {movies_file}.")
        print("Please ensure your data CSV contains an 'id' column for identification.")
        return

    soup_candidate_cols = ['reviewText', 'criticName', 'publicatioName', 'scoreSentiment']
    found_soup_cols = [col for col in soup_candidate_cols if col in actual_existing_cols]

    if not found_soup_cols:
        print(f"Error: None of the primary columns for content soup ({soup_candidate_cols}) were found in {movies_file}.")
        print(f"Found columns for processing: {actual_existing_cols}. Cannot create meaningful content soup. Exiting.")
        return
    elif len(found_soup_cols) < 2:
         print(f"Warning: Only found {len(found_soup_cols)} column(s) for content soup: {found_soup_cols}. Recommendations might be very limited.")

    movies_df = movies_df_full[actual_existing_cols].copy()
    print(f"Movies_df shape after selecting columns: {movies_df.shape}")
    print(f"Columns selected for processing: {movies_df.columns.tolist()}")

    # --- Rename 'id' to 'title' for standardization ---
    if 'id' in movies_df.columns:
        movies_df.rename(columns={'id': 'title'}, inplace=True)
        print("Renamed 'id' column to 'title' for internal processing.")
    else:
        print("Error: 'id' column not found for renaming. This is unexpected as it was checked before.")
        return

    # --- Handle Missing Values ---
    text_feature_cols_for_soup = ['reviewText', 'criticName', 'publicatioName', 'scoreSentiment', 'isTopCritic']
    for col in text_feature_cols_for_soup:
        if col in movies_df.columns:
            movies_df[col] = movies_df[col].fillna('')

    # --- Remove Duplicates based on the new 'title' column ---
    # If 'title' (originally 'id' from the reviews CSV) represents a movie ID and there are multiple reviews
    # for that movie, this keeps only the FIRST review's data for that movie.
    # If 'id' from the reviews CSV is a unique reviewId, this step might not remove much unless there are duplicate reviewIds.
    movies_df.drop_duplicates(subset=['title'], keep='first', inplace=True)
    print(f"Movies_df shape after dropping duplicates based on 'title' (unique IDs): {movies_df.shape}")

    # --- Feature Engineering: Create 'content_soup' ---
    print("\nStep 2: Creating content soup for each item...")
    movies_df['content_soup'] = movies_df.apply(create_content_soup, axis=1)

    movies_df = movies_df[movies_df['content_soup'] != '']
    print(f"Movies_df shape after creating and filtering soup: {movies_df.shape}")

    if movies_df.empty:
        print("Error: No data left after preprocessing. This often means the content_soup was empty for all items.")
        print(f"Please check that your CSV file ('{os.path.basename(movies_file)}') contains meaningful data in the columns used for the soup.")
        return

    # --- TF-IDF Vectorization ---
    print("\nStep 3: Performing TF-IDF Vectorization...")
    tfidf_vectorizer = TfidfVectorizer(stop_words='english', ngram_range=(1,2), min_df=1, max_df=0.8)
    tfidf_matrix = tfidf_vectorizer.fit_transform(movies_df['content_soup'])
    print(f"TF-IDF matrix shape: {tfidf_matrix.shape}")

    if tfidf_matrix.shape[0] == 0 or tfidf_matrix.shape[1] == 0 :
        print("Error: TF-IDF matrix is empty. This might be due to very little unique text in the content soup or too strict TF-IDF parameters.")
        vocabulary_size = len(tfidf_vectorizer.get_feature_names_out()) if hasattr(tfidf_vectorizer, 'get_feature_names_out') else 'N/A (old scikit-learn version)'
        print(f"Vocabulary size: {vocabulary_size}")
        return

    # --- Cosine Similarity ---
    print("\nStep 4: Calculating Cosine Similarity Matrix...")
    cosine_sim_matrix = cosine_similarity(tfidf_matrix, tfidf_matrix)
    print(f"Cosine similarity matrix shape: {cosine_sim_matrix.shape}")

    # --- Prepare data for serving ---
    movies_df_indexed = movies_df[['title']].copy()
    movies_df_indexed.reset_index(drop=True, inplace=True)

    title_to_indices = pd.Series(movies_df_indexed.index, index=movies_df_indexed['title'])

    # --- Save Artifacts ---
    print("\nStep 5: Saving artifacts...")
    np.save('cosine_sim_matrix.npy', cosine_sim_matrix)
    movies_df_indexed.to_parquet('movies_df_indexed.parquet')
    joblib.dump(title_to_indices, 'title_to_indices.joblib')

    print("\nPreprocessing complete. Artifacts saved:")
    print("- cosine_sim_matrix.npy")
    print("- movies_df_indexed.parquet")
    print("- title_to_indices.joblib")

if __name__ == '__main__':
    main()